{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theotrical"
      ],
      "metadata": {
        "id": "TCWqApaplhnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Q.What is Logistic Regression, and how does it differ from Linear\n",
        "   Regression?\n",
        "==> Logistic regression is used for predicting categorical outcomes\n",
        "   (like yes/no, true/false) while linear regression predicts continuous outcomes, using a linear relationship between variables, while logistic regression uses a logistic function to model probabilities.\n",
        "   Linear Regression is used for Regression problems, whereas Logistic Regression is mainly used for solving classification problems.\n",
        "\n",
        "2Q.What is the mathematical equation of Logistic Regression?\n",
        "==>The mathematical equation for logistic regression models the\n",
        "   probability of a binary outcome (0 or 1) based on a linear combination of input features, using the sigmoid function: P(Y=1|X) = 1 / (1 + exp(-(β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ))).\n",
        "\n",
        "3Q.Why do we use the Sigmoid function in Logistic Regression?\n",
        "==>The sigmoid function is used in logistic regression to map the\n",
        "   model's output, which is a linear combination of input features, to a probability between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "4Q.What is the cost function of Logistic Regression?\n",
        "==>In logistic regression, the cost function, also known as log loss\n",
        "   or cross-entropy loss, measures the difference between predicted probabilities and actual class labels, and is calculated as the average of the loss over all training.\n",
        "\n",
        "5Q.What is Regularization in Logistic Regression? Why is it needed?\n",
        "==>Regularization in logistic regression is a technique used to\n",
        "   prevent overfitting by adding a penalty term to the cost function, which reduces the complexity of the model and improves its generalization ability. Because:-\n",
        " * High Dimensionality\n",
        " * Improved Generalization\n",
        " * Stability\n",
        " * Balancing Bias and Variance.\n",
        "\n",
        "6Q.Explain the difference between Lasso, Ridge, and Elastic Net     regression?\n",
        "==>Lasso regression uses L1 regularization, shrinking coefficients to\n",
        "  zero for feature selection, while Ridge regression uses L2 regularization, shrinking coefficients towards zero but not to zero, and Elastic Net combines both L1 and L2 penalties for a balance between feature selection and coefficient shrinkage.\n",
        "  1. Ridge Regression (L2 Regularization):\n",
        "  * Purpose:\n",
        "            Primarily aims to reduce the variance of the model by shrinking coefficients towards zero, preventing overfitting.\n",
        "  * Penalty:\n",
        "            Adds a penalty term to the loss function that is proportional to the sum of the squared coefficients (L2 penalty).\n",
        "  * Effect:\n",
        "           Reduces the magnitude of coefficients, but typically doesn't force them to become exactly zero, thus retaining all features in the model.\n",
        "\n",
        " 2. Lasso Regression (L1 Regularization):\n",
        " * Purpose:\n",
        "           Performs both regularization (reducing variance) and feature selection by shrinking coefficients towards zero.\n",
        " * Penalty:\n",
        "           Adds a penalty term to the loss function that is proportional to the sum of the absolute values of the coefficients (L1 penalty).\n",
        " * Effect:\n",
        "          Can force some coefficients to become exactly zero, effectively eliminating those features from the model.\n",
        "  \n",
        " 3. Elastic Net Regression (L1 and L2 Regularization):\n",
        " * Purpose:\n",
        "          Combines the strengths of both Lasso and Ridge, offering a balance between feature selection and coefficient shrinkage.\n",
        " * Penalty:\n",
        "          Adds a penalty term that is a combination of the L1 and L2 penalties, allowing you to control the balance between feature selection and coefficient shrinkage.\n",
        " * Effect:\n",
        "          Can shrink coefficients towards zero (like Lasso) and also reduce the magnitude of coefficients (like Ridge).\n",
        "\n",
        "7Q.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "==>when you have correlated features and need both feature selection\n",
        "   and handling multicollinearity.\n",
        " * Elastic Net: Combines the best of both worlds (Lasso and Ridge)  \n",
        "                for correlated features and when you need both feature selection and multicollinearity handling.\n",
        " * Lasso: Focuses on feature selection and creating sparse models.\n",
        " * Ridge: Focuses on handling multicollinearity and stabilizing\n",
        "          coefficient estimates.\n",
        "\n",
        "8Q.What is the impact of the regularization parameter (λ) in Logistic\n",
        "   Regression?\n",
        "==>In logistic regression, the regularization parameter (λ) controls\n",
        "   the strength of the penalty applied to the model's coefficients, influencing the model's complexity and preventing overfitting by shrinking coefficients towards zero, thereby improving generalization performance.\n",
        "\n",
        "9Q.What are the key assumptions of Logistic Regression?\n",
        "==>The key assumptions of logistic regression include independence of\n",
        "   errors, linearity in the logit for continuous variables, absence of multicollinearity, and the absence of strongly influential outliers.\n",
        "\n",
        "10Q.What are some alternatives to Logistic Regression for\n",
        "  classification tasks?\n",
        "==>For classification tasks, alternatives to logistic regression\n",
        "   include decision trees, random forests, support vector machines (SVMs), and neural networks.\n",
        " 1. Decision Trees:\n",
        " 2. Random Forests:\n",
        " 3. Support Vector Machines (SVMs)\n",
        " 4. Neural Networks:\n",
        "\n",
        "11Q.What are Classification Evaluation Metrics?\n",
        "==>Classification Evaluation Metrics like accuracy,precision,         recall are good ways to evaluate classification models for balanced\n",
        "datasets, but if the data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance.\n",
        "\n",
        "12Q.How does class imbalance affect Logistic Regression?\n",
        "==>Class imbalance in logistic regression, where one class\n",
        "   significantly outnumbers the other, leads to biased parameter estimates and poor performance, especially for the minority class, as the model tends to favor the majority class.\n",
        "   Here's a more detailed explanation:-\n",
        "  * Biased Parameter Estimates:\n",
        "                               When data is imbalanced, logistic regression models tend to overestimate the coefficients for the majority class and underestimate them for the minority class.\n",
        "  * Poor Performance on the Minority Class:\n",
        "                                          The model becomes less accurate in predicting the minority class, as it's trained primarily on the majority class data.\n",
        "  * Focus on Overall Accuracy:\n",
        "                              Logistic regression, by default, aims to maximize overall accuracy, which can lead to a model that is good at predicting the majority class but fails to identify the minority class.\n",
        "  * Misleading Probability Estimates:\n",
        "                                     The predicted probabilities for the minority class can be skewed, making it difficult to interpret the model's confidence in its predictions.\n",
        "\n",
        "13Q.What is Hyperparameter Tuning in Logistic Regression?\n",
        "==>Hyperparameters are settings or parameters of a machine learning\n",
        "   algorithm that are not learned from the data during training, but are set before training begins. They control the behavior of the model and its training process.\n",
        "\n",
        "14Q.What are different solvers in Logistic Regression? Which one\n",
        "   should be used?\n",
        "==>In scikit-learn's Logistic Regression, the solver parameter\n",
        "   determines the optimization algorithm used to find the best model parameters. For small datasets, 'liblinear' or 'lbfgs' are good choices, while for large datasets, 'sag' or 'saga' are often faster.\n",
        "  * liblinear:\n",
        "              Suitable for small datasets and when fine control over regularization is needed. It supports both L1 and L2 regularization and is a good default choice.\n",
        "\n",
        "15Q.How is Logistic Regression extended for multiclass classification?\n",
        "==>Logistic Regression can be In the multinomial method, the logistic\n",
        "   function is replaced with the softmax function which calculates the probability value of the class over n different classes. The only way to implement the multinomial method with logistic regression is to specify the multi_class=\"multinomial\" argument.2\n",
        "\n",
        "16Q.What are the advantages and disadvantages of Logistic Regression?\n",
        "==>Logistic regression, a simple yet powerful model, excels in binary\n",
        "  classification tasks, offering ease of implementation and interpretation, but struggles with complex, non-linear relationships and can be prone to overfitting.\n",
        " #Advantages:\n",
        "* Simplicity and Ease of Implementation:Logistic regression is a\n",
        "  straightforward model, requiring less computational power and expertise to set up and train compared to more complex algorithms like neural networks.\n",
        "* Interpretability:The coefficients in logistic regression directly\n",
        "  indicate the direction and strength of the relationship between independent variables and the probability of the outcome, making it easy to understand the model's predictions.\n",
        "* Efficiency:Logistic regression is computationally efficient, both  \n",
        "  in terms of training time and prediction speed, especially for large datasets.\n",
        "* Probabilistic Output:The model outputs probabilities, allowing for  \n",
        "  a nuanced understanding of the likelihood of an outcome, rather than just a binary classification.\n",
        "* Handles Various Data Types:Logistic regression can effectively\n",
        "  handle both categorical and continuous data types as inputs.\n",
        "* Extensible to Multiclass Classification:The model can be extended  \n",
        "  to handle multiple classes using techniques like multinomial logistic regression.\n",
        "\n",
        "#Disadvantages:\n",
        "* Assumes Linear Relationships:Logistic regression assumes a linear\n",
        "  relationship between the independent variables and the log-odds of the outcome, which may not hold true for complex datasets.\n",
        "* Sensitivity to Outliers and Multicollinearity:The model can be\n",
        "  sensitive to outliers and multicollinearity (high correlation between independent variables), potentially leading to inaccurate predictions.\n",
        "* Limited for Complex Relationships:Logistic regression struggles  \n",
        "  with complex, non-linear relationships and may not capture intricate patterns in the data.\n",
        "* Potential for Overfitting:In high-dimensional datasets, logistic\n",
        "  regression can be prone to overfitting, where the model learns the training data too well and performs poorly on unseen data.\n",
        "* Requires Feature Selection:Including irrelevant or noisy features\n",
        "  can lead to overfitting and reduce model performance, making feature selection crucial.\n",
        "* Assumption of Independence:The model assumes independence among\n",
        "  predictor variables, which may not always hold true in real-world data.\n",
        "\n",
        "17Q.What are some use cases of Logistic Regression?\n",
        "==>Here some common use cases:-\n",
        "1. Healthcare:\n",
        "  * Predicting Disease Risk:\n",
        "                          Logistic regression can model the likelihood of a patient developing a specific disease based on factors like age, medical history, and lifestyle.\n",
        "  * Diagnosing Conditions:\n",
        "                          It can help determine if a tumor is benign or malignant, or if a patient is likely to have a certain condition based on test results.\n",
        "2. Finance:\n",
        "  * Fraud Detection:\n",
        "                    Logistic regression can identify fraudulent transactions by analyzing patterns in transaction data.\n",
        "  * Credit Risk Assessment:\n",
        "                           It helps predict the likelihood of a loan applicant defaulting on their payment.\n",
        "  * Stock Market Prediction:\n",
        "                            It can be used to predict stock price movements or the probability of a stock going up or down.\n",
        "\n",
        "3. Marketing:\n",
        "  * Customer Churn Prediction:\n",
        "                             It can identify customers who are likely to cancel their subscriptions or stop using a service.\n",
        "  * Targeted Advertising:\n",
        "                         It can predict which customers are most likely to respond to a particular marketing campaign.\n",
        "\n",
        "18Q.What is the difference between Softmax Regression and Logistic\n",
        "   Regression?\n",
        "==>While logistic regression calculates the probability of belonging\n",
        "   to one class, softmax regression calculates the probabilities for each class and normalizes them using the softmax function, ensuring they sum up to one.\n",
        "\n",
        "19Q.How do we choose between One-vs-Rest (OvR) and Softmax for\n",
        "  multiclass classification?\n",
        "==>For multiclass classification, you choose between One-vs-Rest  \n",
        "  (OvR) and Softmax based on your specific needs and the model's capabilities: OvR trains separate binary classifiers for each class, while Softmax directly outputs probabilities for all classes.\n",
        "\n",
        "20Q.How do we interpret coefficients in Logistic Regression?\n",
        "==>In logistic regression, coefficients represent the change in the   log-odds of the outcome for a one-unit change in the predictor variable, and exponentiating these coefficients yields the odds ratio, which shows the change in the odds of the event occurring."
      ],
      "metadata": {
        "id": "Vtg65xDUmC5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRACTICAL"
      ],
      "metadata": {
        "id": "rb6sHdQ0zKJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "x = np.arange(1, 25).reshape(12, 2)\n",
        "y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh9O9H30zUv9",
        "outputId": "9cbdecb0-1ebf-4433-d90d-48b3af6cc338"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# classify small against large digits\n",
        "y = (y > 4).astype(int)\n",
        "\n",
        "l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\n",
        "\n",
        "fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "# Set regularization parameter\n",
        "for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n",
        "    # Increase tolerance for short training time\n",
        "    clf_l1_LR = LogisticRegression(C=C, penalty=\"l1\", tol=0.01, solver=\"saga\")\n",
        "    clf_l2_LR = LogisticRegression(C=C, penalty=\"l2\", tol=0.01, solver=\"saga\")\n",
        "    clf_en_LR = LogisticRegression(\n",
        "        C=C, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=l1_ratio, tol=0.01\n",
        "    )\n",
        "    clf_l1_LR.fit(X, y)\n",
        "    clf_l2_LR.fit(X, y)\n",
        "    clf_en_LR.fit(X, y)\n",
        "\n",
        "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
        "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
        "    coef_en_LR = clf_en_LR.coef_.ravel()\n",
        "\n",
        "    # coef_l1_LR contains zeros due to the\n",
        "    # L1 sparsity inducing norm\n",
        "\n",
        "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
        "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
        "    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n",
        "\n",
        "    print(f\"C={C:.2f}\")\n",
        "    print(f\"{'Sparsity with L1 penalty:':<40} {sparsity_l1_LR:.2f}%\")\n",
        "    print(f\"{'Sparsity with Elastic-Net penalty:':<40} {sparsity_en_LR:.2f}%\")\n",
        "    print(f\"{'Sparsity with L2 penalty:':<40} {sparsity_l2_LR:.2f}%\")\n",
        "    print(f\"{'Score with L1 penalty:':<40} {clf_l1_LR.score(X, y):.2f}\")\n",
        "    print(f\"{'Score with Elastic-Net penalty:':<40} {clf_en_LR.score(X, y):.2f}\")\n",
        "    print(f\"{'Score with L2 penalty:':<40} {clf_l2_LR.score(X, y):.2f}\")\n",
        "\n",
        "    if i == 0:\n",
        "        axes_row[0].set_title(\"L1 penalty\")\n",
        "        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)\n",
        "        axes_row[2].set_title(\"L2 penalty\")\n",
        "\n",
        "    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n",
        "        ax.imshow(\n",
        "            np.abs(coefs.reshape(8, 8)),\n",
        "            interpolation=\"nearest\",\n",
        "            cmap=\"binary\",\n",
        "            vmax=1,\n",
        "            vmin=0,\n",
        "        )\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "\n",
        "    axes_row[0].set_ylabel(f\"C = {C}\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "sxuoURVX8T2s",
        "outputId": "ac62a7a4-dd13-428a-c587-bf931e4aef4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.00\n",
            "Sparsity with L1 penalty:                4.69%\n",
            "Sparsity with Elastic-Net penalty:       4.69%\n",
            "Sparsity with L2 penalty:                4.69%\n",
            "Score with L1 penalty:                   0.90\n",
            "Score with Elastic-Net penalty:          0.90\n",
            "Score with L2 penalty:                   0.90\n",
            "C=0.10\n",
            "Sparsity with L1 penalty:                29.69%\n",
            "Sparsity with Elastic-Net penalty:       12.50%\n",
            "Sparsity with L2 penalty:                4.69%\n",
            "Score with L1 penalty:                   0.90\n",
            "Score with Elastic-Net penalty:          0.90\n",
            "Score with L2 penalty:                   0.90\n",
            "C=0.01\n",
            "Sparsity with L1 penalty:                84.38%\n",
            "Sparsity with Elastic-Net penalty:       68.75%\n",
            "Sparsity with L2 penalty:                4.69%\n",
            "Score with L1 penalty:                   0.86\n",
            "Score with Elastic-Net penalty:          0.88\n",
            "Score with L2 penalty:                   0.89\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "### Logistic regression with ridge penalty (L2) ###\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd # Assuming you'll use pandas to load data\n",
        "\n",
        "# Load your dataset\n",
        "dataset_path = '/path/to/your/dataset.csv'\n",
        "data= pd.read_csv(dataset_path)\n",
        "\n",
        "# Assuming 'target_variable' is the name of your target column\n",
        "X = data.drop('target_variable', axis=1)\n",
        "y = data['target_variable']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42) # Adjust test_size and random_state as needed\n",
        "\n",
        "log_reg_l2_sag = LogisticRegression(penalty='l2', solver='sag', n_jobs=-1)\n",
        "log_reg_l2_sag.fit(xtrain, ytrain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "CUaD0CLK-J55",
        "outputId": "7e7ced77-d0ac-4bb9-e501-3ee8789ccd9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/your/dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-11ca4176483a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/path/to/your/dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Assuming 'target_variable' is the name of your target column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "# Step 1: Import packages, functions, and classes\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Step 2: Get data\n",
        "x = np.arange(10).reshape(-1, 1)\n",
        "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Step 3: Create a model and train it\n",
        "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
        "model.fit(x, y)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "p_pred = model.predict_proba(x)\n",
        "y_pred = model.predict(x)\n",
        "score_ = model.score(x, y)\n",
        "conf_m = confusion_matrix(y, y_pred)\n",
        "report = classification_report(y, y_pred)"
      ],
      "metadata": {
        "id": "8F2mdtaJBG_O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "new_data = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
        "y_pred = model.predict(new_data)\n",
        "print(\"Prediction:\", y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5FYaryIB7Ko",
        "outputId": "39ebebac-db03-4a78-bf77-f9f67ee0373e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Prediction: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpQEqzxNCkO6",
        "outputId": "6621f778-b9e5-4264-c9be-986ac1400c53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "sk_folds = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = sk_folds)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvoypNlUCx5h",
        "outputId": "e890e8e2-97e9-4fad-c924-a487f98561e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
            "Average CV Score:  0.9533333333333334\n",
            "Number of CV Scores used in Average:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "# Replace 'your_dataset.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# 2. Separate features (X) and target variable (y)\n",
        "# Assuming 'target_column' is the name of the target column\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# 3. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8FpmctcODXdI",
        "outputId": "26553a9b-6233-40bd-fa41-89fc071bcaaf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'iris.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9b15b140aed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Replace 'your_dataset.csv' with the actual path to your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 2. Separate features (X) and target variable (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42, n_informative=15, n_redundant=5)\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n",
        "}\n",
        "\n",
        "# 4. Create a Logistic Regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# 5. Perform Randomized Search CV\n",
        "random_search = RandomizedSearchCV(logreg, param_grid, scoring='accuracy', cv=5, n_iter=10, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print the best parameters\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "\n",
        "# 7. Make predictions on the test set using the best model\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# 8. Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am6SfQgGDylQ",
        "outputId": "d3edd649-6a3c-49a6-f57e-55e0f63e36bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 10}\n",
            "Accuracy: 0.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.495   0.77625 0.80375 0.80375     nan 0.80375     nan 0.805       nan\n",
            " 0.805  ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "def preprocess_data(p_train_data, p_test_data, ex_class=None):\n",
        "    if ex_class is not None:\n",
        "        class_filter = np.transpose(p_train_data[:,4]==ex_class)\n",
        "        other_filter = np.transpose(p_train_data[:,4]!=ex_class)\n",
        "        p_train_data[other_filter.A1,4]=0.\n",
        "        p_train_data[class_filter.A1,4]=1.\n",
        "        #print(to_zero_ones(p_train_data[:,4]))\n",
        "\n",
        "    x = p_train_data[:,0:4]\n",
        "    y = p_train_data[:,4]\n",
        "    xtest = p_test_data[:,0:4]\n",
        "    ytest = p_test_data[:,4]\n",
        "\n",
        "    ones = np.ones((x.shape[0],1)).astype(np.float)\n",
        "    onestest = np.ones((xtest.shape[0],1)).astype(np.float)\n",
        "    one_x = np.append(ones, x, 1).astype(np.float)\n",
        "    one_xtest = np.append(onestest, xtest, 1).astype(np.float)\n",
        "\n",
        "    m, n = one_x.shape\n",
        "    theta = np.zeros((1,n)).astype(np.float)\n",
        "    theta = theta.T\n",
        "    return x, y, xtest, ytest, one_x, one_xtest, m, n, theta"
      ],
      "metadata": {
        "id": "5tWIU5DbEA2n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# 1. Load and Prepare Data\n",
        "# Replace 'your_data.csv' with your actual data file\n",
        "# Example:\n",
        "# data = pd.read_csv('your_data.csv')\n",
        "# X = data.drop('target_variable', axis=1)\n",
        "# y = data['target_variable']\n",
        "\n",
        "# For demonstration, create a sample dataset\n",
        "np.random.seed(42)  # for reproducibility\n",
        "X = np.random.rand(100, 2)  # 100 samples, 2 features\n",
        "y = (X[:, 0] + X[:, 1] > 1).astype(int) # Create a binary target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train the Logistic Regression Model\n",
        "# Create and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make Predictions\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate the Model\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# 5. Create and Visualize the Confusion Matrix\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "4DykUeZ0FA0l",
        "outputId": "5d72ad79-2869-4614-8194-f0f7f768dcdf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1, 2], [2, 3], [3, 1], [4, 2], [5, 3], [6, 1], [7, 2], [8, 3]])\n",
        "y = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBIlr7sFL5l",
        "outputId": "d23a03a8-58a0-41e6-c9df-c7a0fa800875"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
        "#Ploting barplot for target\n",
        "plt.figure(figsize=(10,6))\n",
        "g = sns.barplot(data['stroke'], data['stroke'], palette='Set1', estimator=lambda x: len(x) / len(data) )\n",
        "\n",
        "#Anotating the graph\n",
        "for p in g.patches:\n",
        "        width, height = p.get_width(), p.get_height()\n",
        "        x, y = p.get_xy()\n",
        "        g.text(x+width/2,\n",
        "               y+height,\n",
        "               '{:.0%}'.format(height),\n",
        "               horizontalalignment='center',fontsize=15)\n",
        "\n",
        "#Setting the labels\n",
        "plt.xlabel('Heart Stroke', fontsize=14)\n",
        "plt.ylabel('Precentage', fontsize=14)\n",
        "plt.title('Percentage of patients will/will not have heart stroke', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "sniBmlQ0FWc-",
        "outputId": "777996a6-5cae-4a80-92ee-f887eb1fb453"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'healthcare-dataset-stroke-data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3df52cd47883>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'healthcare-dataset-stroke-data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#Ploting barplot for target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'healthcare-dataset-stroke-data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ERzNm_hF4tm"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}